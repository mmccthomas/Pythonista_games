# detection of shapes in an image
# it assumes that obects are closed contours.
# Many of the routines have beeb generated by Gemini 2.5, and # Claude Sonnet 4.5
import numpy as np
from numpy.lib.stride_tricks import sliding_window_view
from PIL import Image, ImageDraw
import os
import math
from operator import attrgetter
import console
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from itertools import cycle
import logging


logging.basicConfig(
    level=logging.INFO,
    format='[%(levelname)s] %(asctime)s %(message)s',
    datefmt='%H:%M:%S'  # This removes the year, month, and day
)
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)  # Set root logger level to DEBUG


def get_distance(p1, p2):
    """Calculates Euclidean distance: sqrt((x2-x1)^2 + (y2-y1)^2)"""
    return math.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)

                
def is_debug_level():
    return logging.getLevelName(logger.getEffectiveLevel()) == 'DEBUG'


def test_plot(coords):
    plt.plot(coords[:, 0], coords[:, 1],
             color='red',
             linewidth=3)
    plt.axis('equal')
    plt.show()

        
class Shape():
    # container for generic shape
    
    def __init__(self, centroid, circularity, coordinates, perimeter):
         
       self.centroid = (int(centroid[0]), int(centroid[1]))
       self.perimeter = int(perimeter)
       self.circularity = round(circularity, 3)
       self.is_circle = 0.7 < circularity < 1.3  # Threshold for "roundness"
       self.coordinates = coordinates
       self.no_points = len(coordinates)
       self.image = None
       self.description = ''
       self.quadrant = ''
       self.image_size = (0, 0)
       self.color_names = ''
       self.shape = 'shape'
       self.pivot = None
       self.length = 0
       self.radius = None
       if self.is_circle:
           self.shape = 'circle'
       if self.circularity == 10:
           self.shape = 'rounded rectangle'
       if self.circularity == -10:
           self.shape = 'rectangle'
       if self.is_circle:            
            self.radius =  int(np.mean(np.linalg.norm(coordinates - np.array(self.centroid), axis=1))) 
    def __repr__(self):
        return (f'{self.quadrant.capitalize()} {self.color_names} '
                f'{self.shape.capitalize()}@{self.centroid} ({self.no_points}points)')

                                
class FeatureExtract():
    """ returns self.img_array, self.edges"""
    def __init__(self, image_path, output_dir='output',
                 canny_low=0.05, canny_high=0.15,
                 edge_tries=1000,
                 size_reduction=1):
                      
        self.image_path = image_path
        self.output_dir = output_dir
        self.canny_low = canny_low
        self.canny_high = canny_high
        self.edge_tries = edge_tries
        self.size_reduction = size_reduction
        self.edges = self.find_edges()
        
    def find_edges(self):
        self.img_array = self.load_image()
        
        # Convert to grayscale
        logger.debug("Converting to grayscale...")
        gray = self.rgb_to_grayscale(self.img_array)
        
        # Edge detection
        logger.debug("Performing edge detection...")
        return self.canny_edge_detection(gray, self.canny_low, self.canny_high, self.edge_tries)
        
    def load_image(self):
        logger.debug(f"Loading image: {self.image_path}")
        
        # Load image
        self.img = Image.open(self.image_path)
       
        self.img = self.img.reduce(self.size_reduction)
        img_array = np.array(self.img)
      
        img_array = img_array[:, :, :3]  # remove alpha
        
        logger.debug(f"Image shape: {img_array.shape}")
        return img_array
        
    def rgb_to_grayscale(self, img):
        """Convert RGB image to grayscale."""
        if len(img.shape) == 2:
            return img
        return np.dot(img[..., :3], [0.299, 0.587, 0.114]).astype(np.uint8)
    
    def gaussian_kernel(self, size, sigma):
        center = size // 2
        # Generate open grids (1xN and Nx1)
        y, x = np.ogrid[-center: size - center, -center: size - center]
        
        # Compute the 2D Gaussian using broadcasting
        kernel = np.exp(-(x**2 + y**2) / (2 * sigma**2))
        
        return kernel / kernel.sum()

    def convolve2d(self, image, kernel):
        h, w = image.shape
        kh, kw = kernel.shape
        pad_h, pad_w = kh // 2, kw // 2
        
        # Pad the image to handle edges
        padded = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w)), mode='edge')
        
        # Create a sliding window view of the image
        # This creates a 4D array of shape (h, w, kh, kw)
        windows = sliding_window_view(padded, (kh, kw))
        
        # Element-wise multiply windows by kernel and sum over the last two axes
        # This replaces the nested for-loops entirely
        output = np.sum(windows * kernel, axis=(2, 3))
        return output.astype(np.float32)
        
    def gaussian_blur(self, image, kernel_size=5, sigma=1.0):
        """Apply Gaussian blur to image."""
        kernel = self.gaussian_kernel(kernel_size, sigma)
        return self.convolve2d(image, kernel)
        
    def sobel_filters(self, image):
        """Apply Sobel filters to get gradients."""
        # Sobel kernels
        Kx = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=np.float32)
        Ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]], dtype=np.float32)
        
        Gx = self.convolve2d(image, Kx)
        Gy = self.convolve2d(image, Ky)
        
        # Gradient magnitude and direction
        G = np.sqrt(Gx**2 + Gy**2)
        theta = np.arctan2(Gy, Gx)
        
        return G, theta
    
    def non_maximum_suppression(self, gradient, theta):
        h, w = gradient.shape
        suppressed = np.zeros_like(gradient)
        
        # Convert angle to degrees and wrap to [0, 180]
        angle = theta * 180.0 / np.pi
        angle[angle < 0] += 180
        
        # Initialize neighbor comparison arrays
        q = np.zeros_like(gradient)
        r = np.zeros_like(gradient)
        
        # Define masks for the four primary directions
        # 0 degrees (Horizontal)
        mask0 = ((0 <= angle) & (angle < 22.5)) | ((157.5 <= angle) & (angle <= 180))
        # 45 degrees (Diagonal /)
        mask45 = (22.5 <= angle) & (angle < 67.5)
        # 90 degrees (Vertical)
        mask90 = (67.5 <= angle) & (angle < 112.5)
        # 135 degrees (Diagonal \)
        mask135 = (112.5 <= angle) & (angle < 157.5)
    
        # Use slicing to shift the image and find neighbors for all pixels at once
        # We ignore the 1-pixel border to match your original loop (1 to h-1)
        
        # Angle 0: Neighbors are Left and Right
        q[1:-1, 1:-1][mask0[1:-1, 1:-1]] = gradient[1:-1, 2:][mask0[1:-1, 1:-1]]
        r[1:-1, 1:-1][mask0[1:-1, 1:-1]] = gradient[1:-1, :-2][mask0[1:-1, 1:-1]]
    
        # Angle 45: Neighbors are Bottom-Left and Top-Right
        q[1:-1, 1:-1][mask45[1:-1, 1:-1]] = gradient[2:, :-2][mask45[1:-1, 1:-1]]
        r[1:-1, 1:-1][mask45[1:-1, 1:-1]] = gradient[:-2, 2:][mask45[1:-1, 1:-1]]
    
        # Angle 90: Neighbors are Top and Bottom
        q[1:-1, 1:-1][mask90[1:-1, 1:-1]] = gradient[2:, 1:-1][mask90[1:-1, 1:-1]]
        r[1:-1, 1:-1][mask90[1:-1, 1:-1]] = gradient[:-2, 1:-1][mask90[1:-1, 1:-1]]
    
        # Angle 135: Neighbors are Top-Left and Bottom-Right
        q[1:-1, 1:-1][mask135[1:-1, 1:-1]] = gradient[:-2, :-2][mask135[1:-1, 1:-1]]
        r[1:-1, 1:-1][mask135[1:-1, 1:-1]] = gradient[2:, 2:][mask135[1:-1, 1:-1]]
    
        # Suppress pixels that are not local maxima
        keep_mask = (gradient >= q) & (gradient >= r)
        suppressed[keep_mask] = gradient[keep_mask]
        return suppressed
    
    def double_threshold(self, image, low_threshold_ratio=0.05, high_threshold_ratio=0.15):
        """Apply double threshold to classify edges."""
        high_threshold = image.max() * high_threshold_ratio
        low_threshold = high_threshold * low_threshold_ratio
        
        strong = 255
        weak = 50
        
        result = np.zeros_like(image)
        
        strong_i, strong_j = np.where(image >= high_threshold)
        weak_i, weak_j = np.where((image <= high_threshold) & (image >= low_threshold))
        
        result[strong_i, strong_j] = strong
        result[weak_i, weak_j] = weak
        
        return result, weak, strong
    
    def edge_tracking(self, image, weak=50, strong=255, edge_tries=10000):
        def dilate(confirmed):
            result = np.copy(confirmed)
            # Cardinal directions
            result[1:, :]  |= confirmed[:-1, :]  # Shift Down
            result[:-1, :] |= confirmed[1:, :]   # Shift Up
            result[:, 1:]  |= confirmed[:, :-1]  # Shift Right
            result[:, :-1] |= confirmed[:, 1:]   # Shift Left
            
            # Diagonals
            result[1:, 1:]   |= confirmed[:-1, :-1]
            result[1:, :-1]  |= confirmed[:-1, 1:]
            result[:-1, 1:]  |= confirmed[1:, :-1]
            result[:-1, :-1] |= confirmed[1:, 1:]
            
            return result
        # Create masks
        strong_mask = (image == strong)
        weak_mask = (image == weak)
        
        # We will "grow" the strong mask into the weak mask
        # Current state of confirmed edges
        confirmed = strong_mask.copy()
        edge_try = 0
        while edge_try < edge_tries:
            # 1. Find all neighbors of currently confirmed strong pixels
            # We shift the image in all 8 directions to simulate the 3x3 window                        
            neighbors = dilate(confirmed)                        
            # 2. A weak pixel becomes confirmed if it touches a confirmed neighbor
            new_confirmations = weak_mask & neighbors & ~confirmed
            
            # 3. If no new pixels were converted, we are done
            if not np.any(new_confirmations):
                break
                
            # 4. Add new pixels to the confirmed set and repeat
            confirmed |= new_confirmations
            edge_try += 1
            
        logger.debug(f'edge tries {edge_try}')
        # Map back to original values
        result = np.zeros_like(image)
        result[confirmed] = strong
        return result
        
    
    def canny_edge_detection(self, image, low_threshold=0.05, high_threshold=0.15, edge_tries=100):
        """Perform Canny edge detection."""
        logger.debug("  Applying Gaussian blur...")
        blurred = self.gaussian_blur(image, kernel_size=5, sigma=1.4)
        
        logger.debug("  Computing gradients...")
        gradient, theta = self.sobel_filters(blurred)
        
        logger.debug("  Non-maximum suppression...")
        suppressed = self.non_maximum_suppression(gradient, theta)
        
        logger.debug("  Double threshold...")
        thresholded, weak, strong = self.double_threshold(suppressed, low_threshold, high_threshold)
        
        logger.debug("  Edge tracking...")                
        edges = self.edge_tracking(thresholded, weak, strong, edge_tries=edge_tries)
        return edges

    def crop_image(self, points):
                        
        # Create a mask for the polygon region
        mask = Image.new('L', self.img.size, 0)
        boundary = [tuple(p) for p in points]
        ImageDraw.Draw(mask).polygon(boundary, outline=255, fill=255)
        
        # Method 2: Crop to bounding box of the polygon
        x_coords = points[:, 0]
        y_coords = points[:, 1]
        bbox = (min(x_coords), min(y_coords), max(x_coords), max(y_coords))
        
        # Crop both image and mask
        cropped_img = self.img.crop(bbox)
        cropped_mask = mask.crop(bbox)
        
        # Apply mask to cropped image
        cropped_rgba = cropped_img.convert('RGBA')
        cropped_rgba_array = np.array(cropped_rgba)
        cropped_mask_array = np.array(cropped_mask)
        cropped_rgba_array[:, :, 3] = cropped_mask_array
        
        result_cropped = Image.fromarray(cropped_rgba_array)
        
        # logger.debug(f"Cropped image size: {result_cropped.size}")
        return result_cropped

    def get_dominant_colors(self, image, k=3, iterations=10):
        """ AI generated
        Get 3 dominant colors"""
        # Load image and resize for performance
        img = image.resize((100, 100))
        img_array = np.array(img)[:, :, :3]  # remove alpha
        pixels = img_array.reshape(-1, 3).astype(float)
        
        # K-Means Implementation
        # Initialize centroids randomly from the existing pixels
        centroids = pixels[np.random.choice(pixels.shape[0], k, replace=False)]
    
        for _ in range(iterations):
            # Calculate Euclidean distance between pixels and centroids
            # (N, 1, 3) - (1, k, 3) -> (N, k, 3) -> distance (N, k)
            distances = np.linalg.norm(pixels[:, np.newaxis] - centroids, axis=2)
            
            # Assign each pixel to the closest centroid
            labels = np.argmin(distances, axis=1)
            
            # Move centroids to the mean of their assigned pixels
            new_centroids = np.array([pixels[labels == i].mean(axis=0)
                                     if np.any(labels == i) else centroids[i]
                                     for i in range(k)])
            
            if np.allclose(centroids, new_centroids):
                break
            centroids = new_centroids
    
        # Calculate percentages
        counts = np.bincount(labels, minlength=k)
        percentages = counts / len(pixels)
        
        # Sort by dominance
        indices = np.argsort(percentages)[::-1]
        return centroids[indices], percentages[indices]

    def closest_colors(self, image):
        """ find the closest colour names to the image
        use k-clustering to identify major colour areas
        """
        def hex_to_rgb(value):
            value = value.lstrip('#')
            return tuple(int(value[i:i+2], 16) for i in (0, 2, 4))

        def find_closest_colourname(rgb):
            # Find the name with the smallest Euclidean distance to the RGB value
            best_name = min(colordict.keys(),
                            key=lambda name: np.linalg.norm(np.array(rgb) - np.array(hex_to_rgb(colordict[name]))))
            return best_name
            
        # colordict = mcolors.TABLEAU_COLORS  # a curated list of colors
        colordict = mcolors.CSS4_COLORS  # a curated list of colors
 
        top_colors, weights = self.get_dominant_colors(image, k=3)
        
        closest_name = []
        for i, (color, weight) in enumerate(zip(top_colors, weights)):
            name = find_closest_colourname(color)
            closest_name.append(name)
            # logger.debug(f"{i+1}. {name}: {weight*100:.1f}% (RGB: {color.astype(int)})")
        # filter black
        closest_name = [name for name in closest_name if name != 'black']
        return '/'.join(closest_name)
        
    def quadrant(self, coord):
        """returns quadrant of coord in image """
        w, h = self.img.size
        results = []
    
        for val, limit, labels in zip(coord, (w, h), [('left', 'centre', 'right'),
                                                      ('top', 'centre', 'bottom')]):
            if val < 0.45 * limit:
                results.append(labels[0])
            elif val > 0.55 * limit:
                results.append(labels[2])
            else:
                results.append(labels[1])
            
        # Swap to return (y_quad, x_quad) as per your original code
        return results[1], results[0]
                 

class FastContourDetector(FeatureExtract):
    """Extends FeatureExtract to add contour detection capability
       returns only closed shapes."""
    def __init__(self, image_path=None, output_dir='output',
                 canny_low=0.05, canny_high=0.15, image_process=None):
       
       if image_process is None:
           super().__init__(image_path, output_dir,
                            canny_low, canny_high)
       else:
           self.edges = image_process.edges
           self.img_array = image_process.img_array
           self.output_dir = image_process.output_dir
    """
    Fast contour detection using parallel labeling approach.
    Pure numpy implementation - no scipy required.
    Much faster than sequential Moore-Neighbor tracing for multiple contours.
    """
    def label_connected_components(self, binary_img):
        """
        Two-pass connected component labeling algorithm (8-connectivity).
        Args:
            binary_img: Binary image (0s and 1s)
            
        Returns:
            labeled_img: Image with each component having a unique label
            num_labels: Number of unique components found
        """
        h, w = self.edges.shape
        labels = np.zeros((h, w), dtype=np.int32)
        next_label = 1
        equivalences = {}  # Union-find structure
        
        def find(x):
            """Find root of equivalence class"""
            if x not in equivalences:
                equivalences[x] = x
            if equivalences[x] != x:
                equivalences[x] = find(equivalences[x])  # Path compression
            return equivalences[x]
        
        def union(x, y):
            """Merge two equivalence classes"""
            root_x, root_y = find(x), find(y)
            if root_x != root_y:
                equivalences[root_y] = root_x
        
        # First pass: assign provisional labels
        for i in range(h):
            for j in range(w):
                if self.edges[i, j] == 0:
                    continue
                
                # Check 8-connected neighbors (only previous ones)
                neighbors = []
                for di, dj in [(-1, -1), (-1, 0), (-1, 1), (0, -1)]:
                    ni, nj = i + di, j + dj
                    if 0 <= ni < h and 0 <= nj < w and labels[ni, nj] > 0:
                        neighbors.append(labels[ni, nj])
                
                if not neighbors:
                    # New component
                    labels[i, j] = next_label
                    next_label += 1
                else:
                    # Assign minimum neighbor label
                    min_label = min(neighbors)
                    labels[i, j] = min_label
                    # Record equivalences
                    for neighbor_label in neighbors:
                        if neighbor_label != min_label:
                            union(min_label, neighbor_label)
        
        # Second pass: resolve equivalences
        label_map = {}
        new_label = 0
        for i in range(h):
            for j in range(w):
                if labels[i, j] > 0:
                    root = find(labels[i, j])
                    if root not in label_map:
                        new_label += 1
                        label_map[root] = new_label
                    labels[i, j] = label_map[root]
        
        return labels, new_label
    
    def binary_erosion(self, img: np.ndarray) -> np.ndarray:
        """
        Fast binary erosion with 3x3 structuring element (8-connectivity).
        
        Args:
            img: Binary image
            
        Returns:
            Eroded binary image
        """
        h, w = img.shape
        eroded = np.zeros_like(img)
        
        # Vectorized erosion using array slicing
        # A pixel is kept only if all 8 neighbors + itself are foreground
        # Create a stack of the 8 neighbors + the center pixel
        shifts = [
            img[0:-2, 0:-2], img[0:-2, 1:-1], img[0:-2, 2:],
            img[1:-1, 0:-2], img[1:-1, 1:-1], img[1:-1, 2:],
            img[2:, 0:-2],   img[2:, 1:-1],   img[2:, 2:]
        ]
        # Logical AND across all shifted versions
        eroded[1:-1, 1:-1] = np.logical_and.reduce(shifts)
        return eroded
    
    def find_all_contours(self, min_contour_length=4, max_contour_length=None):
        """
        Find all contours in a binary image using parallel labeling.
        
        Args:
            min_contour_length: Minimum number of pixels to be considered a contour
            max_contour_length: Maximum number of pixels to be considered a contour
        Returns:
            List of contours, where each contour is an Nx2 array of (row, col) coordinates
        
        """
        # Ensure binary image (0 and 1)
        binary_img = (self.edges > 0).astype(np.uint8)
        
        # Step 1: Label all connected components
        labeled_img, num_features = self.label_connected_components(binary_img)
        
        if num_features == 0:
            return []
        
        # Step 2: Extract boundary pixels for all components at once
        # Boundary = original XOR eroded (gives outer boundary)
        eroded = self.binary_erosion(binary_img)
        boundaries = binary_img ^ eroded  # XOR gives boundary pixels
        
        # Step 3: Extract contours for each labeled component
        contours = []
        
        for label_id in range(1, num_features + 1):
            # Get all boundary pixels for this component
            component_boundary = (labeled_img == label_id) & (boundaries == 1)
            boundary_coords = np.column_stack(np.where(component_boundary))
            boundary_coords = np.fliplr(boundary_coords)  # swap x and y
            if len(boundary_coords) < min_contour_length:
                continue
            if max_contour_length and len(boundary_coords) > max_contour_length:
                continue
            contours.append(boundary_coords)
        
        return contours
    
    def find_all_contours_ordered(self, min_contour_length=4, max_contour_length=None):
        """
        Find all contours with ordered boundary pixels (proper chain).
        Slower than find_all_contours but gives contours in tracing order.
        
        Args:
            min_contour_length: Minimum number of pixels to be considered a contour
            max_contour_length: Maximum number of pixels to be considered a contour
                                primarily for debugging
            
        Returns:
            List of ordered contours
        """
        contours = self.find_all_contours(min_contour_length, max_contour_length)
        ordered_contours = []
        
        for contour in contours:
            if len(contour) == 0:
                continue
            
            ordered = self._order_contour_points(contour, max_neighbour_distance=3)
            if len(ordered) > 0:
               ordered_contours.append(ordered)
        
        return ordered_contours

    def _order_contour_points(self, points, max_neighbour_distance=1.0):
        if len(points) <= 1:
            return points
    
        n_points = len(points)
        ordered = np.zeros_like(points)
        
        mask = np.ones(n_points, dtype=bool)
        
        # Start with the first point
        current_idx = 0
        start_point = points[current_idx]
        ordered[0] = start_point
        mask[current_idx] = False
        
        points_placed = 1
    
        for i in range(1, n_points):
            current = ordered[i-1]
            
            # Early Exit Logic
            # Check if current point is close to start_point
            # and we have traveled at least half the total points
            if i > n_points / 2:
                dist_to_start = np.sum(np.abs(current - start_point))
                if dist_to_start < max_neighbour_distance:
                    # Return only the portion of the array we actually filled
                    return ordered[:i]

            remaining_indices = np.where(mask)[0]
            if len(remaining_indices) == 0:
                break
                
            remaining_points = points[remaining_indices]
            
            # Manhattan distance calculation
            dists = np.sum(np.abs(remaining_points - current), axis=1)
            
            local_idx = np.argmin(dists)
            current_idx = remaining_indices[local_idx]
            
            ordered[i] = points[current_idx]
            mask[current_idx] = False
            points_placed += 1
            
        # filter non closed arcs
        dist_to_start = np.sum(np.abs(current - start_point))
        if dist_to_start > max_neighbour_distance:
            return []
        return ordered[:points_placed]
            
    def pca(self, coords):
        """ unrotate a shape using Principal_Component_Analysis"""
        # Center the coordinates
        centroid = coords.mean(axis=0)
        centered = coords - centroid
       
        # Use PCA to find principal axes
        cov_matrix = np.cov(centered.T)
        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
        
        # Sort by eigenvalue (largest first)
        idx = eigenvalues.argsort()[::-1]
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]
        
        # Rotate points to align with principal axes
        rotated = centered @ eigenvectors
        return rotated, eigenvalues, eigenvectors, centroid
        
    def inverse_pca(self, rotated, eigenvectors, centroid):
        """ Restore original coordinates from PCA-rotated data """
        # 1. Reverse the rotation
        # (Since eigenvectors are orthonormal, the inverse is the transpose)
        unrotated = rotated @ eigenvectors.T
        
        # 2. Add the centroid back
        original_coords = unrotated + centroid
        
        return original_coords
        
    def is_rounded_rectangle(self, coords, tolerance=0):
        """
        Detect if coordinates form a rounded rectangle at any angle using only numpy.
        
        Parameters:
        coords: numpy array of shape (N, 2) with x,y coordinates
        """
        rectangle = False
        if len(coords) < 10:
            return False, False
        
        rotated, eigenvalues, _, _ = self.pca(coords)
        
        # Get bounding box in rotated coordinates
        x_min, y_min = rotated.min(axis=0)
        x_max, y_max = rotated.max(axis=0)
        width = x_max - x_min
        height = y_max - y_min
        
        if width == 0 or height == 0:
            return False, False
        
        # 1. Check edge alignment in rotated coordinates
        edge_threshold = 0.1 * min(width, height)
        
        left_edge = np.sum(np.abs(rotated[:, 0] - x_min) < edge_threshold)
        right_edge = np.sum(np.abs(rotated[:, 0] - x_max) < edge_threshold)
        bottom_edge = np.sum(np.abs(rotated[:, 1] - y_min) < edge_threshold)
        top_edge = np.sum(np.abs(rotated[:, 1] - y_max) < edge_threshold)
        
        edge_points = left_edge + right_edge + bottom_edge + top_edge
        edge_ratio = edge_points / len(coords)
        
        # Most points should be near edges
        # if so it must be a rectangle
        if edge_ratio < 0.7:
            return False, False
        else:
            rectangle = True
        # 2. Check corners for rounding
        corner_threshold = 0.2 * min(width, height)
        corners_rounded = 0
                
        # try a different routine
        # find radius of end
        # find if all coordinates to left of radius are close to circle
        radius_l = x_min + height / 2
        radius_r = x_max - height/2
        # slice left coordinates
        left_end = rotated[rotated[:, 0] < radius_l]
        right_end = rotated[rotated[:, 0] > radius_r]
        distances_l = np.sqrt((left_end[:, 0] - radius_l)**2 + left_end[:, 1]**2)
        total_l = distances_l - height / 2
        close_l = np.all(total_l < corner_threshold)
        distances_r = np.sqrt((right_end[:, 0] - radius_r)**2 + right_end[:, 1]**2)
        total_r = (distances_r - height / 2)
        close_r = np.all(total_r < corner_threshold)
        corners_rounded = 2*close_l + 2*close_r
        
        # 3. Check aspect ratio
        aspect_ratio = max(width, height) / min(width, height)
        
        # 4. Check eigenvalue ratio (should be significantly different for rectangles)
        eigenvalue_ratio = eigenvalues[0] / eigenvalues[1] if eigenvalues[1] > 0 else 0
        is_rectangle = all((rectangle,
                            edge_ratio > 0.7,
                            aspect_ratio < 20,
                            eigenvalue_ratio > 1.5))  # Rectangle has directional variance
        # At least 3 corners rounded, good edge alignment
        is_rounded_rect = all((corners_rounded >= 3,
                               edge_ratio > 0.7,
                               aspect_ratio < 20,
                               eigenvalue_ratio > 1.5))  # Rectangle has directional variance
        
        return is_rounded_rect, is_rectangle

    def analyze_shapes(self, features):
        results = []
        
        for pixels in features:
            # Area is simply the number of pixels in the component
            area = len(pixels)
            
            # Calculate Centroid (Average Y, Average X)
            sum_r = sum(p[0] for p in pixels)
            sum_c = sum(p[1] for p in pixels)
            centroid = (sum_r / area, sum_c / area)
            
            # Calculate Perimeter (Rough estimate: pixels with at least one black neighbor)
            # For simplicity, we'll use a bounding box approach or pixel count
            # A more accurate perimeter uses the outer edge pixels
            perimeter = self.estimate_perimeter(pixels)
            
            # Calculate Euclidean distance from centroid
            # axis=1 calculates the norm for each row (point)
            distances = np.linalg.norm(pixels - np.array(centroid), axis=1)
            radius = np.mean(distances)
            span = np.var(distances) / radius
            if span < 1:
               circularity = 1 - span
            else:
               circularity = 0
            rounded_rectangle, rectangle = self.is_rounded_rectangle(pixels, tolerance=0.05)
            if rounded_rectangle:
               circularity = 10
            elif rectangle:
                circularity = -10
            results.append(Shape(
                centroid=(int(centroid[0]), int(centroid[1])),
                perimeter=int(perimeter),
                circularity=round(circularity, 3),
                coordinates=pixels
            ))
        return results

    def estimate_perimeter(self, pixels):
        if len(pixels) == 0:
            return 0
    
        # 1. Normalize and move to a grid
        pixels = np.unique(pixels, axis=0)
        min_coords = pixels.min(axis=0)
        shifted_pixels = pixels - min_coords
        
        # Create grid with 1-pixel padding to avoid index errors during shifts
        shape = shifted_pixels.max(axis=0) + 3
        grid = np.zeros(shape, dtype=bool)
        grid[shifted_pixels[:, 0] + 1, shifted_pixels[:, 1] + 1] = True
        
        # 2. Check 4-way neighbors by shifting the grid
        # We look for pixels that are True, but have a False neighbor
        up = grid[:-2, 1:-1]
        down = grid[2:, 1:-1]
        left = grid[1:-1, :-2]
        right = grid[1:-1, 2:]
        center = grid[1:-1, 1:-1]
        
        # A pixel is on the edge if it's True AND any neighbor is False
        # (center) & ~(all neighbors are True)
        interior = up & down & left & right
        edge_mask = center & ~interior
        
        return np.sum(edge_mask)
                
    def plot_contours(self, shapes, color=None, linewidth=None):
        if linewidth is None:
          linewidth = 3
        # Create output directory
        os.makedirs(self.output_dir, exist_ok=True)
        
        if shapes is not None:
            # Draw lines on original image
            output_img = Image.fromarray(self.img_array)
            draw = ImageDraw.Draw(output_img)
            
            logger.debug("Detected contours:")
            
            color_iter = cycle([
                (0, 255, 0),    # Green
                (255, 0, 0),    # Red
                (0, 0, 255),    # Blue
                (255, 165, 0),  # Orange
                (128, 0, 128),  # Purple
                (0, 255, 255),  # Cyan
                (255, 192, 203),  # Pink
                (165, 42, 42),  # Brown
            ])
            
            for shape in shapes:
               contour = shape.coordinates
               match shape.shape:
                   case 'circle':
                       color = 'red'
                   case 'rounded rectangle':
                       color = 'cyan'
                   case 'teardrop':
                       color = 'purple'
                       
                   case _:
                       color = 'green'
               
               if color is None:
                  color = next(color_iter)
               # contour = np.fliplr(contour)
               # Draw each segment
               total_length = 0
               p0 = contour[0]
               for p1 in contour[1:]:
                   draw.line([tuple(p0), tuple(p1)], fill=color, width=linewidth)
                   p0 = p1
                   total_length += get_distance(p0, p1)
               if shape.shape == 'teardrop':
                   pivot = shape.pivot
                   pivot_box = [tuple(pivot - np.array([5, 5])), tuple(pivot + np.array([5, 5]))]
                   draw.ellipse(pivot_box, fill=color, outline=color, width=2)
                                              
            # Save marked image
            marked_filename = os.path.join(self.output_dir, 'contours_detected.png')
            output_img.save(marked_filename)
            logger.debug(f"Saved marked image: {marked_filename}")
            
            # Save edges
            edge_filename = os.path.join(self.output_dir, 'edges.png')
            Image.fromarray(self.edges.astype(np.uint8)).save(edge_filename)
            logger.debug(f"Saved edge image: {edge_filename}")
            
    def filter_duplicates(self, shapes, threshold=5.0):
        """ removes shapes with same centroid, leaving the largest
        shape is {'centroid': (1594, 176), 'no_points': 230, 'perimeter': 230, 'circularity': 0.055, 'is_circle': False} """
    
        if not shapes:
            return []
    
        # Sort by size descending so the largest version of a shape is encountered first
        # This makes the comparison logic much cleaner
        sorted_shapes = sorted(shapes, key=attrgetter('no_points'), reverse=True)
        unique_shapes = []
    
        for s in sorted_shapes:
            is_duplicate = False
            c1 = s.centroid
            
            for u in unique_shapes:
                c2 = u.centroid
                # Euclidean distance: sqrt((x2-x1)^2 + (y2-y1)^2)
                dist = math.sqrt((c1[0] - c2[0])**2 + (c1[1] - c2[1])**2)
                
                if dist <= threshold:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_shapes.append(s)
                
        return unique_shapes
        
    def calculate_shape_features(self, coords):
        """Calculate geometric features to identify the shape
        produced by Claude Sonnet 4.5."""
        x, y = coords[:, 0], coords[:, 1]
        
        # Center the data
        x_center = np.mean(x)
        y_center = np.mean(y)
        x_centered = x - x_center
        y_centered = y - y_center
        
        # Calculate distances from center
        distances = np.sqrt(x_centered**2 + y_centered**2)
        
        # Calculate angles
        angles = np.arctan2(y_centered, x_centered)
        
        # Sort by angle to analyze shape progression
        sorted_indices = np.argsort(angles)
        sorted_distances = distances[sorted_indices]
        sorted_angles = angles[sorted_indices]
        sorted_x = x_centered[sorted_indices]
        sorted_y = y_centered[sorted_indices]
        
        # Feature 1: Coefficient of variation of distances (for circle detection)
        distance_std = np.std(distances)
        distance_mean = np.mean(distances)
        cv_distance = distance_std / distance_mean if distance_mean > 0 else 0
        
        # Feature 2: Aspect ratio (width to height ratio)
        width = np.max(x) - np.min(x)
        height = np.max(y) - np.min(y)
        aspect_ratio = width / height if height > 0 else 0
        
        # Feature 3: Smoothness - check for corners (rectangles have sharp transitions)
        # Calculate second derivative approximation (curvature)
        # Use larger window to reduce noise sensitivity
        window = 10
        if len(sorted_x) > 2 * window:
            # Calculate tangent directions using centered differences
            tangent_angles = []
            for i in range(window, len(sorted_x) - window):
                dx = sorted_x[i + window] - sorted_x[i - window]
                dy = sorted_y[i + window] - sorted_y[i - window]
                tangent_angles.append(np.arctan2(dy, dx))
            
            tangent_angles = np.array(tangent_angles)
            angle_changes = np.abs(np.diff(tangent_angles))
            # Wrap around for angles near ±π
            angle_changes = np.minimum(angle_changes, 2*np.pi - angle_changes)
        else:
            dx = np.diff(sorted_x)
            dy = np.diff(sorted_y)
            angles_diff = np.arctan2(dy, dx)
            angle_changes = np.abs(np.diff(angles_diff))
            angle_changes = np.minimum(angle_changes, 2*np.pi - angle_changes)
        
        max_angle_change = np.max(angle_changes)
        mean_angle_change = np.mean(angle_changes)
        # Std of angle changes - high for rectangles (concentrated corners)
        std_angle_change = np.std(angle_changes)
        
        # Feature 4: Test for four-fold symmetry (rectangles)
        # Check if distances are consistent within quadrants
        quadrant_distances = []
        for i in range(4):
            angle_start = -np.pi + i * np.pi/2
            angle_end = -np.pi + (i+1) * np.pi/2
            mask = (sorted_angles >= angle_start) & (sorted_angles < angle_end)
            if np.sum(mask) > 0:
                quadrant_distances.append(np.mean(sorted_distances[mask]))
        
        if len(quadrant_distances) == 4:
            quadrant_std = np.std(quadrant_distances)
            quadrant_variation = quadrant_std / np.mean(quadrant_distances)
        else:
            quadrant_variation = 0
        
        # Feature 5: Bounding box area vs actual area (rectangularity test)
        # Approximate area using shoelace formula
        area_shoelace = 0.5 * np.abs(np.sum(sorted_x[:-1] * sorted_y[1:] - sorted_x[1:] * sorted_y[:-1]))
        area_bbox = width * height
        area_ratio = area_shoelace / area_bbox if area_bbox > 0 else 0
        
        # Feature 6: Check for corner detection
        # In rectangles, there should be 4 points with high curvature
        # Find peaks in angle changes
        threshold = np.percentile(angle_changes, 95)
        num_corners = np.sum(angle_changes > threshold)
        
        # Feature 7: Teardrop detection - check for asymmetry in end radii
        # Better approach: measure the vertical extent (height) at each end
        # rather than radial distance from center
        
        # Divide shape into segments along x-axis
        x_min_val, x_max_val = np.min(x), np.max(x)
        x_range = x_max_val - x_min_val
        
        # Left 10% of x range (left end)
        left_threshold = x_min_val + 0.1 * x_range
        left_end_mask = x <= left_threshold
        
        # Right 10% of x range (right end)
        right_threshold = x_max_val - 0.1 * x_range
        right_end_mask = x >= right_threshold
        
        # Measure vertical extent at each end
        if np.sum(left_end_mask) > 0:
            left_y_values = y[left_end_mask]
            left_end_height = np.max(left_y_values) - np.min(left_y_values)
        else:
            left_end_height = 0
        
        if np.sum(right_end_mask) > 0:
            right_y_values = y[right_end_mask]
            right_end_height = np.max(right_y_values) - np.min(right_y_values)
        else:
            right_end_height = 0
        
        # Calculate end height ratio (larger / smaller)
        if left_end_height > 0 and right_end_height > 0:
            end_height_ratio = max(left_end_height, right_end_height) / min(left_end_height, right_end_height)
            end_height_asymmetry = abs(left_end_height - right_end_height) / max(left_end_height, right_end_height)
        else:
            end_height_ratio = 1.0
            end_height_asymmetry = 0.0
        
        # Also keep the original radial measurements as backup
        # Divide the shape into left and right halves
        x_mid = (x_max_val + x_min_val) / 2
        
        # Left half
        left_mask = x < x_mid
        left_distances = distances[left_mask]
        left_radius_mean = np.mean(left_distances) if len(left_distances) > 0 else 0
        
        # Right half
        right_mask = x >= x_mid
        right_distances = distances[right_mask]
        right_radius_mean = np.mean(right_distances) if len(right_distances) > 0 else 0
        
        radius_asymmetry = abs(left_radius_mean - right_radius_mean) / max(left_radius_mean, right_radius_mean) if max(left_radius_mean, right_radius_mean) > 0 else 0
        
        return {
            'cv_distance': cv_distance,
            'aspect_ratio': aspect_ratio,
            'max_angle_change': max_angle_change,
            'mean_angle_change': mean_angle_change,
            'std_angle_change': std_angle_change,
            'quadrant_variation': quadrant_variation,
            'area_ratio': area_ratio,
            'num_corners': num_corners,
            'width': width,
            'height': height,
            'mean_distance': distance_mean,
            'radius_asymmetry': radius_asymmetry,
            'end_height_ratio': end_height_ratio,
            'end_height_asymmetry': end_height_asymmetry,
            'left_end_height': left_end_height,
            'right_end_height': right_end_height,
            'left_radius_mean': left_radius_mean,
            'right_radius_mean': right_radius_mean
        }
        
    def identify_teardrop(self, features, min_size_ratio=0.003, stats=False):
        """Identify the a teardrop based on calculated features.
        modified from AI Claude Sonnet 4.5 code to only find
        teardrop of large enough size"""
        
        cv = features['cv_distance']
        aspect_ratio = features['aspect_ratio']
        max_angle = features['max_angle_change']
        std_angle = features['std_angle_change']
        area_ratio = features['area_ratio']
        num_corners = features['num_corners']
        end_height_ratio = features['end_height_ratio']
        end_height_asymmetry = features['end_height_asymmetry']
        radius_asymmetry = features['radius_asymmetry']
        area = features['width'] * features['height']
        image_area = np.prod(self.img_array.shape[:2])
        image_area_ratio = area / image_area
        if stats:
            logger.debug("\n" + "="*70)
            logger.debug("SHAPE IDENTIFICATION USING NUMPY")
            logger.debug("="*70)
            logger.debug("\nGeometric Features Calculated:")
            logger.debug(f"  {'Feature':<30} {'Value':<15} {'Interpretation'}")
            logger.debug(f"  {'-'*30} {'-'*15} {'-'*25}")
            logger.debug(f"  {'Coefficient of Variation':<30} {cv:.4f}{'':>11} {'Distance consistency'}")
            logger.debug(f"  {'Aspect Ratio (W/H)':<30} {aspect_ratio:.4f}{'':>11} {'Shape elongation'}")
            logger.debug(f"  {'Max Angle Change':<30} {max_angle:.4f}{'':>11} {'Corner sharpness'}")
            logger.debug(f"  {'Std Angle Change':<30} {std_angle:.4f}{'':>11} {'Curvature variance'}")
            logger.debug(f"  {'Image Area Ratio (Shape/BBox)':<30} {image_area_ratio:.4f}{'':>11} {'Space filling'}")
            logger.debug(f"  {'Area Ratio (Shape/BBox)':<30} {area_ratio:.4f}{'':>11} {'Space filling'}")
            logger.debug(f"  {'Number of Corners':<30} {num_corners:<15} {'Sharp transitions'}")
            logger.debug(f"  {'End Height Ratio':<30} {end_height_ratio:.4f}{'':>11} {'End asymmetry'}")
            logger.debug(f"  {'End Height Asymmetry %':<30} {end_height_asymmetry:.4f}{'':>11} {'Vertical extent diff'}")
            logger.debug(f"  {'Radius Asymmetry':<30} {radius_asymmetry:.4f}{'':>11} {'Left vs Right'}")
            logger.debug(f"  {'Left End Height':<30} {features['left_end_height']:.2f}{'':>11}")
            logger.debug(f"  {'Right End Height':<30} {features['right_end_height']:.2f}{'':>11}")
            logger.debug(f"  {'Width':<30} {features['width']:.2f}{'':>11}")
            logger.debug(f"  {'Height':<30} {features['height']:.2f}{'':>11}")
            
            logger.debug("\n" + "-"*70)
            logger.debug("CLASSIFICATION LOGIC:")
            logger.debug("-"*70)
        
        # Decision thresholds
        # Rectangles have very high std (concentrated corners at 90 degrees) and high area ratio
        RECTANGLE_STD_ANGLE_THRESHOLD = 0.5  # Very high std indicates 4 concentrated corners
        MINIMUM_AREA_RATIO = min_size_ratio
        ELLIPSE_ASPECT_THRESHOLD = 1.5
        # Teardrop has asymmetric ends - use vertical height measurements
        TEARDROP_HEIGHT_RATIO_THRESHOLD = 1.5  # One end at least 50% taller than the other
        TEARDROP_HEIGHT_ASYMMETRY_THRESHOLD = 0.20  # At least 20% difference in end heights
        
        # Classify
        # Teardrop: elongated with asymmetric end heights
        is_teardrop = all((image_area_ratio > MINIMUM_AREA_RATIO,
                           aspect_ratio > ELLIPSE_ASPECT_THRESHOLD,
                           std_angle < RECTANGLE_STD_ANGLE_THRESHOLD,
                           any((end_height_ratio > TEARDROP_HEIGHT_RATIO_THRESHOLD,
                                end_height_asymmetry > TEARDROP_HEIGHT_ASYMMETRY_THRESHOLD))
                          ))

        if stats:
            logger.debug("\nTeardrop Test:")
            logger.debug(f"  • Image Area ratio? area_ratio = {image_area_ratio:.4f} > {MINIMUM_AREA_RATIO}")
            logger.debug(f"  • Elongated? aspect_ratio = {aspect_ratio:.4f} > {ELLIPSE_ASPECT_THRESHOLD} = {aspect_ratio > ELLIPSE_ASPECT_THRESHOLD}")
            logger.debug(f"  • Smooth curves? std_angle = {std_angle:.4f} < {RECTANGLE_STD_ANGLE_THRESHOLD} = {std_angle < RECTANGLE_STD_ANGLE_THRESHOLD}")
            logger.debug(f"  • Asymmetric end heights? end_height_ratio = {end_height_ratio:.4f} > {TEARDROP_HEIGHT_RATIO_THRESHOLD} = {end_height_ratio > TEARDROP_HEIGHT_RATIO_THRESHOLD}")
            logger.debug(f"  • OR end_height_asymmetry = {end_height_asymmetry:.4f} > {TEARDROP_HEIGHT_ASYMMETRY_THRESHOLD} = {end_height_asymmetry > TEARDROP_HEIGHT_ASYMMETRY_THRESHOLD}")
            logger.debug(f"  → Is Teardrop: {is_teardrop}")
                        
            logger.debug("\n" + "="*70)
            logger.debug("FINAL IDENTIFICATION:")
            logger.debug("="*70)
        
        if is_teardrop:
            shape = "TEARDROP"
            reasoning = [
                f"Significantly elongated (aspect ratio {aspect_ratio:.2f}:1)",
                f"Asymmetric ends: height ratio = {end_height_ratio:.2f}:1",
                f"Left end height: {features['left_end_height']:.1f}, Right end height: {features['right_end_height']:.1f}",
                f"One end is {end_height_asymmetry:.1%} larger than the other",
                f"Smooth, continuous curves (std angle = {std_angle:.4f})",
                "Rounded at one end, more pointed at the other"
            ]
        
        else:
            shape = "UNKNOWN/IRREGULAR"
            reasoning = ["Shape doesn't match standard geometric forms"]
        if stats:
            logger.debug(f"\n✓ Shape Identified: {shape}\n")
            logger.debug("Reasoning:")
            for i, reason in enumerate(reasoning, 1):
                logger.debug(f"  {i}. {reason}")
            
            logger.debug("\n" + "="*70 + "\n")
        
        return shape
        
    def process_teardrop(self, shape):
        """ find the pivot and length of a teardrop"""
        # unrotate the shape to centre on 0,0 in E-W direction
        rotated, eigenvalues, eigenvectors, centroid = self.pca(shape.coordinates)
        features = self.calculate_shape_features(rotated)
        if self.identify_teardrop(features, stats=False) == 'TEARDROP':
         
            shape.shape = 'teardrop'
            # compute pivot from radius of thicker end
            max_width = max(features['left_end_height'], features['right_end_height'])
            x1 = features['width'] / 2 - max_width / 2
            if features['left_end_height'] > features['right_end_height']:
                x1 = -x1
            # rotate back to place pivot.
            # convert to array to process, then get only item
            xy = np.array([(x1, 0)])
            shape.pivot = self.inverse_pca(xy, eigenvectors, centroid)[0].astype(int)
            # length from pivot to thinner end
            shape.length = int(features['width'] - np.abs(x1))


def get_shapes(file_path):
    image_path = file_path
    output_dir = 'output'
    canny_low = 0.05  # Lower = more edges (more noise)
    canny_high = 0.15  # Lower = more edges (more noise)
    min_contour_length = 130
    max_contour_length = None
    min_spacing = 10
    size_reduction = 4
    image_process = FeatureExtract(image_path, output_dir,
                                   canny_low, canny_high,
                                   edge_tries=1000, size_reduction=size_reduction)
    
    detector = FastContourDetector(image_process=image_process)
    logger.debug("Finding ordered contours...")
    ordered_contours = detector.find_all_contours_ordered(min_contour_length,
                                                          max_contour_length)
    shapes = detector.analyze_shapes(ordered_contours)
    shapes = detector.filter_duplicates(shapes, threshold=min_spacing)
    logger.debug(f"Found {len(ordered_contours)} ordered contours")
    
    logger.debug(f"Found {len(shapes)} filtered contours")
    for i, shape in enumerate(shapes):
        # test_plot(shape.coordinates)
        detector.process_teardrop(shape)
        image = image_process.crop_image(shape.coordinates)
        shape.image = image
        shape.image_size = image.size
        shape.color_names = image_process.closest_colors(image)
        shape.quadrant = '_'.join(image_process.quadrant(shape.centroid))
        shape.description = f"{shape.quadrant} {shape.color_names} {shape.shape}"
        logger.debug(f"{i}: {shape}")
        image.show()
    detector.plot_contours(shapes, color=None, linewidth=10)
    marked_filename = os.path.join(output_dir, 'contours_detected.png')
    #console.quicklook(marked_filename)
    
    return shapes
    
                                                                                                                                                                                                                          
def main():
   get_shapes('pinball1.png')

        
if __name__ == '__main__':
   #for i in range(7,8):
   #   shapes = get_shapes(f'pinball{i}.png')
   # main()
   # 
   import cProfile
   cProfile.run('get_shapes("pinball7.png")', sort='cumulative')
"""
Common Issues
“No circles detected”
  1.  Check edge image: output/edges.png
  • Are circle edges visible?
  • If not, adjust canny_low and canny_high
  2.  Lower threshold: Try threshold=5
  3.  Adjust radius range to match your circles
  4.  Check image quality (blur, noise, occlusion)
“Too many circles detected”
  1.  Raise threshold: Try threshold=15 or higher
  2.  Increase min_dist
  3.  Narrow radius range
  4.  Improve edge detection (higher Canny thresholds)
“Circles in wrong locations”
  • Edge detection may be finding other circular features
  • Check edges.png to verify circle edges are clean
  • Adjust Canny parameters for better edge quality
Performance Notes
Circle detection is computationally intensive:
  • Time increases with: (max_radius - min_radius) × num_edge_points
  • For large images or wide radius ranges, expect 10-60 seconds
  • The vectorized implementation is already optimized
"""

